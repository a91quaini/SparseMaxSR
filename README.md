# SparseMaxSR.jl

**SparseMaxSR** is a Julia package for *meanâ€“variance efficient (MVE)* portfolio selection under **cardinality constraints**.  
It provides unified, numerically robust tools to estimate **sparse maximumâ€‘Sharpe portfolios**, i.e., portfolios composed of at most or exactly *k* assets out of *N*, while preserving the classical meanâ€“variance structure.

---

## 1. Problem statement

We seek to solve the *sparse Sharpeâ€‘maximization problem*:

$$
\max_{w \in \mathbb{R}^N}
\frac{w' \mu}{\sqrt{w' \Sigma w}}
\quad \text{s.t.} \quad \|w\|_0 \le k,
$$

where

- $\mu \in \mathbb{R}^N$: vector of expected excess returns,  
- $\Sigma \in \mathbb{R}^{N\times N}$: covariance matrix of returns,  
- $\|w\|_0$: number of nonzero elements in $w$ (sparsity).

The unconstrained MVE (meanâ€“variance efficient) portfolio is

$$
w_{\text{MVE}} = \Sigma^{-1}\mu, \qquad
SR_{\text{MVE}} = \sqrt{\mu'\Sigma^{-1}\mu}.
$$

SparseMaxSR provides algorithms that approximate this solution when the support size is restricted.

---

## 2. Methods implemented

SparseMaxSR offers three complementary approaches, all sharing a unified API:

| Method | Description | Typical use |
|:--------|:-------------|:-------------|
| **Exhaustive / Random Search** | Enumerates or samples subsets of size *k* and selects the one with maximal inâ€‘sample MVE Sharpe ratio. | Small N (â‰¤Â 30â€“40) or validation of heuristics. |
| **LASSO Relaxation Search** | Solves a continuous relaxation via **Elasticâ€‘Net (GLMNet)** regression; selects the largest support â‰¤Â *k* and optionally refits exact MVE weights. | Large N, fast approximation. |
| **MIQP Heuristic Search** | Mixedâ€‘integer quadratic heuristic via **JuMPÂ +Â CPLEX**, with optional cardinality band and progressive bound expansion. | Medium/large N; high accuracy with time control. |

All methods call the shared lowâ€‘level routines in the [`SharpeRatio`](#sharperatio-module) and [`Utils`](#utils-module) modules for stable covariance handling and Sharpe computation.

---

## 3. Installation

### OptionÂ AÂ â€” local development

```julia
julia> using Pkg
julia> Pkg.activate(".")
julia> Pkg.develop(path="/path/to/SparseMaxSR")
julia> using SparseMaxSR
```

### OptionÂ BÂ â€” clone from Git

```julia
julia> using Pkg
julia> Pkg.add(url="https://github.com/<org>/SparseMaxSR.jl")
julia> using SparseMaxSR
```

> Add solver backends (`CPLEX`, `HiGHS`, `MosekTools`, `SCS`, etc.) to your environment as required.

---

## 4. Dependencies

| Dependency | Purpose |
|:------------|:---------|
| `LinearAlgebra`, `Statistics`, `Random` | Core numerical routines |
| `Combinatorics` | Exhaustive and random subset generation |
| `GLMNet` | LASSO / Elasticâ€‘Net path solver |
| `JuMP`, `MathOptInterface`, `CPLEX` | MIQP heuristic search |

---

## 5. Exported API

All main routines return **named tuples** `(selection, weights, sr, status)` or `(selection, sr)` depending on context.

| Field | Meaning |
|:------|:---------|
| `selection` | Vector of indices of selected assets |
| `weights` | Portfolio weights (zeros offâ€‘support) |
| `sr` | Inâ€‘sample Sharpe ratio |
| `status` | Symbol describing solver outcome (`:OK`, `:LASSO_PATH_EXACT_K`, etc.) |

---

### ðŸ”¹ SharpeRatio module

#### `compute_sr`

```julia
compute_sr(w::AbstractVector, Î¼::AbstractVector, Î£::AbstractMatrix;
           selection::AbstractVector{<:Integer}=Int[],
           epsilon::Real=EPS_RIDGE,
           stabilize_Î£::Bool=true,
           do_checks::Bool=false) -> Float64
```

Computes the Sharpe ratio of a given portfolio:

$$
SR(w) = \frac{w' \mu}{\sqrt{w' (\Sigma + \epsilon I) w}}.
$$

If `selection` is provided, only those indices contribute to the numerator and denominator.  
Returns `NaN` when variance â‰¤Â 0 or not finite.

---

#### `compute_mve_sr`

```julia
compute_mve_sr(Î¼::AbstractVector, Î£::AbstractMatrix;
               selection::AbstractVector{<:Integer}=Int[],
               epsilon::Real=EPS_RIDGE,
               stabilize_Î£::Bool=true,
               do_checks::Bool=false) -> Float64
```

Computes the **maximum Sharpe ratio** of the meanâ€“variance efficient (MVE) portfolio on a given subset:

$$
SR^*(S) = \sqrt{ \mu_S' \Sigma_S^{-1} \mu_S }.
$$

When `selection` is empty, the fullâ€‘universe MVE Sharpe ratio is returned.

---

#### `compute_mve_weights`

```julia
compute_mve_weights(Î¼::AbstractVector, Î£::AbstractMatrix;
                    selection::AbstractVector{<:Integer}=Int[],
                    normalize_weights::Bool=false,
                    epsilon::Real=EPS_RIDGE,
                    stabilize_Î£::Bool=true,
                    do_checks::Bool=false) -> Vector{Float64}
```

Computes MVE weightsÂ $wÂ =Â \Sigma^{-1}\mu$ (restricted to the chosen support if provided).  
If `normalize_weights=true`, weights are rescaled for numerical stability.  
Normalization does **not** affect Sharpe ratios (scaleâ€‘invariant).

---

### ðŸ”¹ ExhaustiveSearch module

#### `mve_exhaustive_search`

```julia
mve_exhaustive_search(Î¼::AbstractVector{<:Real},
                      Î£::AbstractMatrix{<:Real};
                      k::Integer,
                      epsilon::Real = Utils.EPS_RIDGE,
                      stabilize_Î£::Bool = true,
                      do_checks::Bool = false,
                      # enumeration / sampling knobs
                      enumerate_all::Bool = true,
                      max_samples::Int = 0,
                      dedup_samples::Bool = true,
                      rng::AbstractRNG = Random.GLOBAL_RNG,
                      # outputs
                      compute_weights::Bool = true
) -> NamedTuple{(:selection, :weights, :sr, :status)}
```

Enumerates or samples subsets of sizeÂ `k`, returning the best subset and its MVE Sharpe ratio.  
For feasibleÂ $\binom{N}{k}$, setÂ `enumerate_all=true`; otherwise, provideÂ `max_samples`.

| Argument | Description |
|:----------|:-------------|
| `Î¼`,Â `Î£` | Mean and covariance |
| `k` | Target subset size |
| `enumerate_all` | Enumerate all combinations (`true`) or sample |
| `max_samples` | Number of samples if not enumerating |
| `dedup_samples` | Ensure sampled supports are unique |
| `rng` | Random number generator |
| `epsilon`,Â `stabilize_Î£`,Â `do_checks` | Numerical options |
| `compute_weights` | Output optimal weights |

Returns `(selection, sr)`.

---

### ðŸ”¹ LassoRelaxationSearch module

#### `mve_lasso_relaxation_search`

```julia
mve_lasso_relaxation_search(R::AbstractMatrix{<:Real};
                            k::Integer,
                            y::Union{Nothing,AbstractVector{<:Real}}=nothing,
                            nlambda::Int=100,
                            lambda_min_ratio::Real=1e-3,
                            alpha::Real=0.95,
                            standardize::Bool=false,
                            epsilon::Real=EPS_RIDGE,
                            stabilize_Î£::Bool=true,
                            compute_weights::Bool=false,
                            normalize_weights::Bool=false,
                            use_refit::Bool=true,
                            do_checks::Bool=false)
    -> NamedTuple{(:selection,:weights,:sr,:status)}
```

**Pathâ€‘based Elasticâ€‘Net relaxation** of the sparse MVE problem:

1. Fits GLMNet pathÂ $yÂ =Â R\betaÂ +Â \varepsilon$.  
2. Chooses the largest supportÂ â‰¤Â `k`.  
3. Either:  
   - `use_refit=true`: compute exact MVE SR and refit weights on the selected support;  
   - `use_refit=false`: return the raw or normalized LASSO coefficients.

**Statuses**:  
`:LASSO_PATH_EXACT_K`,Â `:LASSO_PATH_ALMOST_K`,Â `:LASSO_ALLEMPTY`.

---

### ðŸ”¹ MIQPHeuristicSearch module

#### `mve_miqp_heuristic_search`

```julia
mve_miqp_heuristic_search(Î¼::AbstractVector, Î£::AbstractMatrix;
                          k::Integer,
                          exactly_k::Bool=false,
                          m::Union{Int,Nothing}=nothing,
                          Î³::Float64=1.0,
                          fmin::AbstractVector=zeros(length(Î¼)),
                          fmax::AbstractVector=ones(length(Î¼)),
                          expand_rounds::Int=20,
                          expand_factor::Float64=3.0,
                          expand_tol::Float64=1e-2,
                          mipgap::Float64=1e-4,
                          time_limit::Real=200,
                          threads::Int=0,
                          compute_weights::Bool=false,
                          normalize_weights::Bool=false,
                          use_refit::Bool=true,
                          verbose::Bool=false,
                          epsilon::Real=EPS_RIDGE,
                          stabilize_Î£::Bool=true,
                          do_checks::Bool=false)
```

Heuristic mixedâ€‘integer quadratic programming (MIQP) solver for sparse MVE selection.  
The optimization problem is:

$$
\begin{aligned}
\min_{x,v} \; & \tfrac{1}{2} \gamma x' \Sigma_s x - \mu'x \\
\text{s.t.} \;& m \le \sum_i v_i \le k, \\
& v_i=0 \Rightarrow x_i=0, \\
& v_i=1 \Rightarrow f_{\min,i} \le x_i \le f_{\max,i}.
\end{aligned}
$$

IfÂ `normalize_weights=true`, adds a budget constraintÂ $\sum_i x_i = 1$Â and rescales outputs.

Key options:
- `expand_rounds`,Â `expand_factor`,Â `expand_tol`: progressive boundâ€‘expansion heuristic  
- `mipgap`,Â `time_limit`: CPLEX tolerances  
- `use_refit`: recompute exact MVE SR/weights on final support  
- `normalize_weights`: toggles âˆ‘xâ€¯=â€¯1 and postâ€‘normalization

Returns `(selection,Â weights,Â sr,Â status)`.

---

## 6. Example usage

The included [`example.jl`](example.jl) script compares all methods on simulated twoâ€‘factor returns:

```bash
julia --project=. example.jl
```

Example output:

```
Results â€” ExperimentÂ AÂ (T=500,Â N=30)
------------------------------------
k      | EXHAUSTIVE         | LASSOâ€‘VANILLA      | LASSOâ€‘REFIT        | MIQPâ€‘VANILLA       | MIQPâ€‘REFIT
-----------------------------------------------------------------------------------------------------
1      | 0.1538Â /Â 0.00s     | 0.1538Â /Â 0.00s     | 0.1538Â /Â 0.00s     | 0.1538Â /Â 4.7s      | 0.1538Â /Â 0.2s
3      | 0.1962Â /Â 0.07s     | 0.1760Â /Â 0.00s     | 0.1945Â /Â 0.00s     | 0.1930Â /Â 3.1s      | 0.1955Â /Â 0.6s
...
```

---

