# SparseMaxSR.jl

**SparseMaxSR** is a Julia package for *meanâ€“variance efficient (MVE)* portfolio selection under **cardinality constraints**.  
It provides unified, numerically robust tools to estimate **sparse maximumâ€‘Sharpe portfolios**, i.e., portfolios composed of at most or exactly *k* assets out of *N*, while preserving the classical meanâ€“variance structure.

---

## 1. Problem statement

We seek to solve the *sparse Sharpeâ€‘maximization problem*:

$$
\max_{w \in \mathbb{R}^N}
\frac{w' \mu}{\sqrt{w' \Sigma w}}
\quad \text{s.t.} \quad \|w\|_0 \le k,
$$

where

- $\mu \in \mathbb{R}^N$: vector of expected excess returns,  
- $\Sigma \in \mathbb{R}^{N\times N}$: covariance matrix of returns,  
- $\|w\|_0$: number of nonzero elements in $w$ (sparsity).

The unconstrained MVE (meanâ€“variance efficient) portfolio is

$$
w_{\text{MVE}} = \Sigma^{-1}\mu, \qquad
SR_{\text{MVE}} = \sqrt{\mu'\Sigma^{-1}\mu}.
$$

SparseMaxSR provides algorithms that approximate this solution when the support size is restricted.

---

## 2. Methods implemented

SparseMaxSR offers three complementary approaches, all sharing a unified API:

| Method | Description | Typical use |
|:--------|:-------------|:-------------|
| **Exhaustive / Random Search** | Enumerates or samples subsets of size *k* and selects the one with maximal inâ€‘sample MVE Sharpe ratio. | Small N (â‰¤Â 30â€“40) or validation of heuristics. |
| **LASSO Relaxation Search** | Solves a continuous relaxation via **Elasticâ€‘Net (GLMNet)** regression; selects the largest support â‰¤Â *k* and optionally refits exact MVE weights. | Large N, fast approximation. |
| **MIQP Heuristic Search** | Mixedâ€‘integer quadratic heuristic via **JuMPÂ +Â CPLEX**, with optional cardinality band and progressive bound expansion. | Medium/large N; high accuracy with time control. |

All methods call the shared lowâ€‘level routines in the [`SharpeRatio`](#sharperatio-module) and [`Utils`](#utils-module) modules for stable covariance handling and Sharpe computation.

---

## 3. Installation

### OptionÂ AÂ â€” local development

```julia
julia> using Pkg
julia> Pkg.activate(".")
julia> Pkg.develop(path="/path/to/SparseMaxSR")
julia> using SparseMaxSR
```

### OptionÂ BÂ â€” clone from Git

```julia
julia> using Pkg
julia> Pkg.add(url="https://github.com/<org>/SparseMaxSR.jl")
julia> using SparseMaxSR
```

> Add solver backends (`CPLEX`, `HiGHS`, `MosekTools`, `SCS`, etc.) to your environment as required.

---

## 4. Dependencies

| Dependency | Purpose |
|:------------|:---------|
| `LinearAlgebra`, `Statistics`, `Random` | Core numerical routines |
| `Combinatorics` | Exhaustive and random subset generation |
| `GLMNet` | LASSO / Elasticâ€‘Net path solver |
| `JuMP`, `MathOptInterface`, `CPLEX` | MIQP heuristic search |

---

## 5. Exported API

All main routines return **named tuples** `(selection, weights, sr, status)` or `(selection, sr)` depending on context.

| Field | Meaning |
|:------|:---------|
| `selection` | Vector of indices of selected assets |
| `weights` | Portfolio weights (zeros offâ€‘support) |
| `sr` | Inâ€‘sample Sharpe ratio |
| `status` | Symbol describing solver outcome (`:OK`, `:LASSO_PATH_EXACT_K`, etc.) |

---

### ðŸ”¹ SharpeRatio module

#### `compute_sr`

```julia
compute_sr(w::AbstractVector, Î¼::AbstractVector, Î£::AbstractMatrix;
           selection::AbstractVector{<:Integer}=Int[],
           epsilon::Real=EPS_RIDGE,
           stabilize_Î£::Bool=true,
           do_checks::Bool=false) -> Float64
```

Computes the Sharpe ratio of a given portfolio:

$$
SR(w) = \frac{w' \mu}{\sqrt{w' (\Sigma + \epsilon I) w}}.
$$

If `selection` is provided, only those indices contribute to the numerator and denominator.  
Returns `NaN` when variance â‰¤Â 0 or not finite.

---

#### `compute_mve_sr`

```julia
compute_mve_sr(Î¼::AbstractVector, Î£::AbstractMatrix;
               selection::AbstractVector{<:Integer}=Int[],
               epsilon::Real=EPS_RIDGE,
               stabilize_Î£::Bool=true,
               do_checks::Bool=false) -> Float64
```

Computes the **maximum Sharpe ratio** of the meanâ€“variance efficient (MVE) portfolio on a given subset:

$$
SR^*(S) = \sqrt{ \mu_S' \Sigma_S^{-1} \mu_S }.
$$

When `selection` is empty, the fullâ€‘universe MVE Sharpe ratio is returned.

---

#### `compute_mve_weights`

```julia
compute_mve_weights(Î¼::AbstractVector, Î£::AbstractMatrix;
                    selection::AbstractVector{<:Integer}=Int[],
                    normalize_weights::Bool=false,
                    epsilon::Real=EPS_RIDGE,
                    stabilize_Î£::Bool=true,
                    do_checks::Bool=false) -> Vector{Float64}
```

Computes MVE weightsÂ $wÂ =Â \Sigma^{-1}\mu$ (restricted to the chosen support if provided).  
If `normalize_weights=true`, weights are rescaled for numerical stability.  
Normalization does **not** affect Sharpe ratios (scaleâ€‘invariant).

---

### ðŸ”¹ ExhaustiveSearch module

#### `mve_exhaustive_search`

```julia
mve_exhaustive_search(Î¼::AbstractVector{<:Real},
                      Î£::AbstractMatrix{<:Real};
                      k::Integer,
                      epsilon::Real = Utils.EPS_RIDGE,
                      stabilize_Î£::Bool = true,
                      do_checks::Bool = false,
                      # enumeration / sampling knobs
                      enumerate_all::Bool = true,
                      max_samples::Int = 0,
                      dedup_samples::Bool = true,
                      rng::AbstractRNG = Random.GLOBAL_RNG,
                      # outputs
                      compute_weights::Bool = true
) -> NamedTuple{(:selection, :weights, :sr, :status)}
```

Enumerates or samples subsets of sizeÂ `k`, returning the best subset and its MVE Sharpe ratio.  
For feasibleÂ $\binom{N}{k}$, setÂ `enumerate_all=true`; otherwise, provideÂ `max_samples`.

| Argument | Description |
|:----------|:-------------|
| `Î¼`,Â `Î£` | Mean and covariance |
| `k` | Target subset size |
| `enumerate_all` | Enumerate all combinations (`true`) or sample |
| `max_samples` | Number of samples if not enumerating |
| `dedup_samples` | Ensure sampled supports are unique |
| `rng` | Random number generator |
| `epsilon`,Â `stabilize_Î£`,Â `do_checks` | Numerical options |
| `compute_weights` | Output optimal weights |

Returns `(selection, sr)`.

---

### LassoRelaxationSearch

The `LassoRelaxationSearch` module implements **LASSO and Elasticâ€‘Net relaxations** of the sparse maximumâ€‘Sharpe portfolio problem.  
It provides two main entry points:

---

#### `mve_lasso_relaxation_search(R::AbstractMatrix; k::Int, Î±::Union{Float64,Vector{Float64}}=1.0, use_refit::Bool=false, ...)`

Perform sparse meanâ€‘variance efficient (MVE) portfolio selection via a **LASSO/Elasticâ€‘Net relaxation** using the *returns matrix* \( R \in \mathbb{R}^{T \times N} \).

##### Arguments

- `R::Matrix{Float64}` â€” matrix of excess returns (rows = time, columns = assets).  
- `k::Int` â€” target support size.  
- `Î±::Union{Float64,Vector{Float64}}=1.0` â€” Elasticâ€‘Net mixing parameter(s):  
  - `Î±=1.0` â†’ pure LASSO;  
  - `Î±<1.0` â†’ Elasticâ€‘Net penalty.  
  - If a **vector** of Î± values is passed, *crossâ€‘validation* is performed automatically.  
- `use_refit::Bool=false` â€” if `true`, the final weights are recomputed by refitting the exact MVE solution on the selected support.  
- `nlambda::Int=100` â€” number of Î» values used internally by GLMNet.  
- `lambda_min_ratio::Float64=1e-3` â€” ratio Î»â‚˜áµ¢â‚™ / Î»â‚˜â‚â‚“.  
- `standardize::Bool=false` â€” whether to standardize predictors (columns of `R`).  
- `normalize_weights::Bool=false` â€” whether to normalize the final weights to sum to one.  
- `weights_sum1::Bool=false` â€” if `true`, enforces \(\sum_i w_i = 1\) in the refit step.  
- `epsilon::Float64` â€” ridgeâ€‘style regularization constant for numerical stability.  
- `stabilize_Î£::Bool` â€” whether to stabilize the sample covariance before inversion.  
- `do_checks::Bool=false` â€” perform argument and dimension checks.  
- `cv_folds::Int=5` â€” number of folds for Î±â€‘grid crossâ€‘validation (if Î± is a vector).  
- `cv_verbose::Bool=false` â€” print crossâ€‘validation progress.

##### Returns

A named tuple with fields:

```julia
(selection = Vector{Int},
 weights    = Vector{Float64},
 sr         = Float64,
 status     = Symbol,
 alpha      = Float64)
```

where  
- `selection` is the index set of chosen assets,  
- `weights` are the corresponding portfolio weights,  
- `sr` is the inâ€‘sample Sharpe ratio,  
- `status` is one of:
  - `:OK` â€” valid selection and Sharpe ratio;
  - `:LASSO_PATH_ALMOST_K` â€” best model had fewer than `k` active coefficients;
  - `:LASSO_ALLEMPTY` â€” all coefficients were zero;
- `alpha` is the chosen Î± (either the input value or the CVâ€‘selected optimum).

---

#### `mve_lasso_relaxation_search(Î¼::Vector, Î£::Matrix, T::Int; R::Union{Nothing,Matrix}=nothing, Î±::Union{Float64,Vector{Float64}}=1.0, ...)`

Momentâ€‘based entry point (using **estimated moments** instead of raw returns).  
This function provides identical functionality but allows passing sample moments directly.

##### Arguments

- `Î¼::Vector{Float64}` â€” mean vector of returns.  
- `Î£::Matrix{Float64}` â€” covariance matrix of returns.  
- `T::Int` â€” effective sample size.  
- `R::Union{Nothing,Matrix}=nothing` â€” optional returns matrix.  
  If provided, Î±â€‘grid CV is performed across the values in `Î±`.  
- All remaining keyword arguments are identical to the previous method.

##### Returns

Same named tuple as above.

---

##### Notes

- When multiple Î± values are provided, `mve_lasso_relaxation_search` internally performs **crossâ€‘validation** on `R` to select the Î± yielding the highest outâ€‘ofâ€‘sample Sharpe ratio, and reports that Î± in the output field `alpha`.  
- Setting `use_refit=true` recomputes the exact MVE weights restricted to the selected support, using `compute_mve_weights` internally.  
- The LASSO and refit procedures can be used for grid experiments over both support size `k` and Î± to approximate the sparse maximumâ€‘Sharpe frontier efficiently.

---

### ðŸ”¹ MIQPHeuristicSearch module

#### `mve_miqp_heuristic_search`

```julia
mve_miqp_heuristic_search(Î¼::AbstractVector, Î£::AbstractMatrix;
                          k::Integer,
                          exactly_k::Bool=false,
                          m::Union{Int,Nothing}=nothing,
                          Î³::Float64=1.0,
                          fmin::AbstractVector=zeros(length(Î¼)),
                          fmax::AbstractVector=ones(length(Î¼)),
                          expand_rounds::Int=20,
                          expand_factor::Float64=3.0,
                          expand_tol::Float64=1e-2,
                          mipgap::Float64=1e-4,
                          time_limit::Real=200,
                          threads::Int=0,
                          compute_weights::Bool=false,
                          normalize_weights::Bool=false,
                          use_refit::Bool=true,
                          verbose::Bool=false,
                          epsilon::Real=EPS_RIDGE,
                          stabilize_Î£::Bool=true,
                          do_checks::Bool=false)
```

Heuristic mixedâ€‘integer quadratic programming (MIQP) solver for sparse MVE selection.  
The optimization problem is:

$$
\begin{aligned}
\min_{x,v} \; & \tfrac{1}{2} \gamma x' \Sigma_s x - \mu'x \\
\text{s.t.} \;& m \le \sum_i v_i \le k, \\
& v_i=0 \Rightarrow x_i=0, \\
& v_i=1 \Rightarrow f_{\min,i} \le x_i \le f_{\max,i}.
\end{aligned}
$$

IfÂ `normalize_weights=true`, adds a budget constraintÂ $\sum_i x_i = 1$Â and rescales outputs.

Key options:
- `expand_rounds`,Â `expand_factor`,Â `expand_tol`: progressive boundâ€‘expansion heuristic  
- `mipgap`,Â `time_limit`: CPLEX tolerances  
- `use_refit`: recompute exact MVE SR/weights on final support  
- `normalize_weights`: toggles âˆ‘xâ€¯=â€¯1 and postâ€‘normalization

Returns `(selection,Â weights,Â sr,Â status)`.

---

## 6. Example usage

The included [`example.jl`](example.jl) script compares all methods on simulated twoâ€‘factor returns:

```bash
julia --project=. example.jl
```

Example output:

```
Results â€” ExperimentÂ AÂ (T=500,Â N=30)
------------------------------------
k      | EXHAUSTIVE         | LASSOâ€‘VANILLA      | LASSOâ€‘REFIT        | MIQPâ€‘VANILLA       | MIQPâ€‘REFIT
-----------------------------------------------------------------------------------------------------
1      | 0.1538Â /Â 0.00s     | 0.1538Â /Â 0.00s     | 0.1538Â /Â 0.00s     | 0.1538Â /Â 4.7s      | 0.1538Â /Â 0.2s
3      | 0.1962Â /Â 0.07s     | 0.1760Â /Â 0.00s     | 0.1945Â /Â 0.00s     | 0.1930Â /Â 3.1s      | 0.1955Â /Â 0.6s
...
```

---

